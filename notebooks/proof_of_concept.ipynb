{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62216b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6908ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dcd1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"hard_hat_on\",\"hard_hat_off\",\"hard_hat_on_test\",\"hard_hat_off_test\"]\n",
    "for label in labels:\n",
    "    if not os.path.exists(label):\n",
    "        os.mkdir(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a39d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press any button to start data collection forhard_hat_on\n",
      " \n",
      "Press any button to start data collection forhard_hat_off\n",
      " \n",
      "Press any button to start data collection forhard_hat_on_test\n",
      "s\n",
      "Press any button to start data collection forhard_hat_off_test\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for folder in labels[:2]:\n",
    "    print(\"Press any button to start data collection for\"+folder)\n",
    "    userinput = input()  \n",
    "    for count in range(200):\n",
    "        status, frame = camera.read()\n",
    "        yep = frame\n",
    "        cv.imshow(\"Video Window\",yep)\n",
    "        frame = cv.resize(yep, (224,224))\n",
    "        cv.imwrite('C:/Users/Cameron/capstone/'+folder+'/img'+str(count)+'.png',yep)\n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            break\n",
    "for folder in labels[2:]:\n",
    "    print(\"Press any button to start data collection for\"+folder)\n",
    "    userinput = input()  \n",
    "    for count in range(20):\n",
    "        status, frame = camera.read()\n",
    "        yep = frame\n",
    "        cv.imshow(\"Video Window\",yep)\n",
    "        frame = cv.resize(yep, (224,224))\n",
    "        cv.imwrite('C:/Users/Cameron/capstone/'+folder+'/img'+str(count)+'.png',yep)\n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            break            \n",
    "# When everything done, release the capture\n",
    "camera.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd2e50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "image = Image.open('C:/Users/Cameron/capstone/hard_hat/img1.png')\n",
    "io.imshow(np.asarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a097d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import imageio\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self,img_dir_true,img_dir_false):\n",
    "        self.img_dirs = [os.path.join(img_dir_true,img) for img in os.listdir(img_dir_true)] + [os.path.join(img_dir_false,img) for img in os.listdir(img_dir_false)]\n",
    "        self.labels = [1]*len(os.listdir(img_dir_true)) + [0]*len(os.listdir(img_dir_false))\n",
    "        print(len(self.labels))\n",
    "        print(len(self.img_dirs))\n",
    "    def __len__(self):\n",
    "        return len(self.img_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):   \n",
    "        batch = {\"data\" : torch.transpose(torch.tensor(imageio.imread(self.img_dirs[idx])),0,2).float() , \"labels\" : torch.tensor(self.labels[idx])}        \n",
    "        return [batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02761f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomImageDataset(img_dir_true = \"C:/Users/Cameron/capstone/hard_hat_on\", img_dir_false = \"C:/Users/Cameron/capstone/hard_hat_off\")\n",
    "test_dataset = CustomImageDataset(img_dir_true = \"C:/Users/Cameron/capstone/hard_hat_on_test\", img_dir_false = \"C:/Users/Cameron/capstone/hard_hat_off_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf574bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [320, 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee0fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model =  torchvision.models.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae9ffb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[1] = torch.nn.Linear(in_features=model.classifier[1].in_features,out_features=1)\n",
    "#model.classifier[2] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34883b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lightning module\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "F = torch.nn.BCEWithLogitsLoss()\n",
    "M = torch.nn.Sigmoid()\n",
    "\n",
    "class HatNoHat(pl.LightningModule):\n",
    "    def __init__(self, train_dataset, val_dataset, model, col_fn, learning_rate=5e-5, num_loading_cpus=1, batch_size=4):\n",
    "        super().__init__() \n",
    "        self.model =  model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.col_fn = col_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_loading_cpus = num_loading_cpus\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def forward(self,x):\n",
    "        f = self.model.forward(x)\n",
    "        return f\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch['data']\n",
    "        #print('x ', x)\n",
    "        y = torch.unsqueeze(batch['labels'],0).float()\n",
    "        #print('y ', y)\n",
    "        y_hat = self.forward(x.float())\n",
    "        print('y_hat ', y_hat)\n",
    "        output = F(y_hat,y)\n",
    "        #print('loss ', output)\n",
    "        return output\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['data']\n",
    "        #print('x ', x)\n",
    "        y = torch.unsqueeze(batch['labels'],0).float()\n",
    "        #print('y ', y)\n",
    "        y_hat = self.forward(x.float())\n",
    "        #print('y_hat ', y_hat)\n",
    "        output = F(y_hat,y)\n",
    "        #print('loss ', loss)\n",
    "        return output\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset,batch_size=self.batch_size,collate_fn=self.col_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset,batch_size=self.batch_size,collate_fn=self.col_fn)     \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "babf2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def col_fn(batch):\n",
    "    #print(batch)\n",
    "    out = dict()\n",
    "    out['data'] = torch.stack([x['data'] for x in batch[0]])\n",
    "    out['labels'] = torch.stack([x['labels'] for x in batch[0]])\n",
    "    #print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef43b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agh = HatNoHat(train_set, val_set, model, col_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d71cc175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | MobileNetV2 | 2.2 M \n",
      "--------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.901     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cameron\\.conda\\envs\\camdefaultenv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Cameron\\.conda\\envs\\camdefaultenv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a156b94b2ebe416fbefdc73ad46c1f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat  tensor([[-0.0706]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1483]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1338]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1103]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2087]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4283]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.0372]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.1371]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0314]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3076]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1407]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4841]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0026]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1687]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1899]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2902]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2471]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1628]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1900]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2378]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2885]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3102]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.1300]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3713]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0240]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1311]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4021]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3958]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.0770]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3873]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.5213]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3690]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4216]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2023]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1742]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3400]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3810]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2320]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.7136]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.5389]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1624]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1495]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.6195]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0307]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.5906]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1531]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2533]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4450]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.0743]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3481]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2988]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0146]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4158]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.2810]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1560]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0779]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2691]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.0110]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.2771]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0116]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.0741]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2254]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.1020]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.2356]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.1521]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.1059]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0842]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3655]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0274]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1054]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2574]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.3046]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.1395]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.7408]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0083]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1353]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.0356]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.0624]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.0533]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.0860]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat  tensor([[0.2544]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4545]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.3039]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.2708]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.6687]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.5266]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2282]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.3060]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.0047]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.2821]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.1937]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4746]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.1790]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.2932]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4412]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2737]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.5497]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.2641]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.4274]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4920]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.6263]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.6671]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.3854]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.7737]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.2130]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.7172]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.8132]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.4384]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.2669]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.0646]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.4151]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.6078]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.9981]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.5320]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.3299]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.9913]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.6507]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.1330]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.1327]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.2820]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.7881]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.1955]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.0866]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.6791]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.8308]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.8026]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.3336]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.7698]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.9408]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.4879]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.5343]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.7607]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.9543]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.6841]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.3291]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.4526]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[0.6815]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-0.8919]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-1.1269]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.2303]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.1524]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.1492]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-1.2801]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-1.1877]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-1.2408]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-1.3572]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-1.4452]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.3815]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-1.3671]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.2072]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.3587]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-1.4228]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-1.5003]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.2974]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[-1.1435]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.1671]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.4110]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.3126]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.3914]], grad_fn=<AddmmBackward0>)\n",
      "y_hat  tensor([[1.2441]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=2)\n",
    "trainer.fit(agh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d63525",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dl = torch.utils.data.DataLoader(val_set,batch_size=1,collate_fn=col_fn)\n",
    "M = torch.nn.Sigmoid()\n",
    "pred = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for i,batch in enumerate(dl):\n",
    "        plt.figure()\n",
    "        #if i == 5:\n",
    "        plt.imshow(np.asarray(torch.transpose(batch['data'].squeeze().long(),0,2)))\n",
    "        plt.show()\n",
    "        x = batch['data']\n",
    "        y = torch.unsqueeze(batch['labels'],0).float()\n",
    "        #print('y ', y)\n",
    "        y_hat = model.forward(x.float())\n",
    "        print('y_hat ', M(y_hat)   )  \n",
    "        if y_hat.numpy().tolist()[0][0] > 0.5:\n",
    "            print('HAT')\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            print(\"NO HAT\")\n",
    "            pred.append(0)\n",
    "        labels.append(y.numpy().tolist()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad64044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "149eeef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.95744681, 1.        ]),\n",
       " array([1.        , 0.94285714]),\n",
       " array([0.97826087, 0.97058824]),\n",
       " array([45, 35], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dl = torch.utils.data.DataLoader(test_dataset,batch_size=1,collate_fn=col_fn)\n",
    "M = torch.nn.Sigmoid()\n",
    "pred = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for i,batch in enumerate(dl):\n",
    "        plt.figure()\n",
    "        #if i == 5:\n",
    "        plt.imshow(np.asarray(torch.transpose(batch['data'].squeeze().long(),0,2)))\n",
    "        plt.show()\n",
    "        x = batch['data']\n",
    "        y = torch.unsqueeze(batch['labels'],0).float()\n",
    "        #print('y ', y)\n",
    "        y_hat = model.forward(x.float())\n",
    "        print('y_hat ', M(y_hat)   )  \n",
    "        if y_hat.numpy().tolist()[0][0] > 0.5:\n",
    "            print('HAT')\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            print(\"NO HAT\")\n",
    "            pred.append(0)\n",
    "        labels.append(y.numpy().tolist()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "016604b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.55555556, 1.        ]),\n",
       " array([1. , 0.2]),\n",
       " array([0.71428571, 0.33333333]),\n",
       " array([20, 20], dtype=int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcc06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
